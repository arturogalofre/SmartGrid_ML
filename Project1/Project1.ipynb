{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***ECE563: AI in Smart Grid***\n",
    "\n",
    "Arturo Galofr√© (A20521022)\n",
    "\n",
    "1. **A brief overview of the project**\n",
    "\n",
    "The field of machine learning has seen tremendous growth in recent years and has led to numerous breakthroughs in a variety of industries. In particular, supervised learning algorithms have been instrumental in solving many real-world problems. This project aims to provide a comprehensive overview of several of the fundamental supervised learning algorithms, including Linear Regression, Logistic Regression, Decision Trees, Support Vector Machines, and k-Nearest Neighbors. By exploring these algorithms, we will gain a deeper understanding of how each of these methods can be used to make predictions based on historical data. We will be working with real-world datasets to demonstrate the strengths and limitations of each algorithm. This project will provide a hands-on approach to learning the basics of supervised machine learning, making it an ideal starting point to begin understanding in this exciting field.\n",
    "\n",
    "2. **Python code that uses the Scikit-Learn Linear Regression algorithm to predict the number of unit sales given an amount of money spent on radio advertising. You can extract the radio advertising data from the Advertising CSV file as follows (there are other approaches, so do what works for you):**\n",
    "\n",
    "- `df = pd.read_csv(\"Advertising.csv\")`\n",
    "- `x_arr = df[\"radio\"].to_numpy()`\n",
    "- `y_arr = df[\"sales\"].to_numpy()`\n",
    "- `x_arr = x_arr.reshape(-1,1)`\n",
    "\n",
    "**Since we are only interested in the best fit curve, you may use all the data for training.**\n",
    "\n",
    "3. **Outputs: State the linear model coefficients determined by Linear Regression and the estimated # of units sold given a radio advertising budget of 23 M$ (units used for the radio advertising data).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing of all necessary libraries for the assingment\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.datasets import load_diabetes, load_breast_cancer\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept:  9.311638095158283\n",
      "Coefficient:  [0.20249578]\n",
      "Estimated number of units sold for 23M$:  [4657412.32966421]\n"
     ]
    }
   ],
   "source": [
    "# Money spent on advertising\n",
    "money_spent = 100\n",
    "\n",
    "# Load advertising data from csv file\n",
    "df = pd.read_csv(\"Advertising.csv\")\n",
    "\n",
    "# Split data into features (x) and target (y)\n",
    "# .reshape array (rows, columns), -1 tells the number of rows is the number of elements in the array\n",
    "# .values converts dataframe to numpy array as skicit only takes data in numpy array form\n",
    "\n",
    "x = df[\"radio\"].values.reshape(-1, 1)\n",
    "y = df[\"sales\"].values\n",
    "\n",
    "# Create a Linear Regression object\n",
    "model = LinearRegression()\n",
    "\n",
    "# Train the model using the training data\n",
    "# The fit method is used to train a machine learning model on a training dataset. The fit method takes two required arguments: the first argument is the training data (also called the \"features\") and the second argument is the target data (also called the \"labels\").\n",
    "model.fit(x, y)\n",
    "\n",
    "# Predict the number of unit sales\n",
    "# .predict predicts an output based on the feature money_spent\n",
    "radio_spend = [[money_spent]]\n",
    "radio_spend = np.array(radio_spend)\n",
    "sales_prediction = model.predict(radio_spend)\n",
    "\n",
    "# Linear model coefficients\n",
    "intercept = model.intercept_\n",
    "coefficient = model.coef_\n",
    "print(\"Intercept: \", intercept)\n",
    "print(\"Coefficient: \", coefficient)\n",
    "\n",
    "# Estimation of the number of units sold for 23M$\n",
    "radio_spend = 23000000\n",
    "sales_prediction = model.predict([[radio_spend]])\n",
    "print(\"Estimated number of units sold for 23M$: \", sales_prediction)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Observations: How well do your coefficients match the results from our Gradient Descent example? Which method is faster for training the model?**\n",
    "\n",
    "The coefficients obtained from the scikit-learn implementation of linear regression should match the results from a gradient descent implementation example viewed in class. Both methods are solving for the same linear regression problem, so the coefficients should be the same. After inspecting the graphs presented in class we can observe the \"y\" intercept is practically the same as the one obtained in the linear reresion model.\n",
    "\n",
    "As for which method is faster for training the model, scikit-learn uses highly optimized linear algebra libraries so it is generally faster than a custom gradient descent implementation. Gradient descent can be slower compared to linear regression when dealing with large datasets due to the iterative nature of the optimization process.\n",
    "\n",
    "5. **Python code that uses the Scikit-Learn Logistic Regression algorithm to predict the presence of cancer based on the processed medical image data in the breast cancer dataset. For this part of the project, we will randomly select a subset of features for our classification problem. Use the following code to adjust the seed for the pseudo-random number generator that will randomly select 4 features:**\n",
    "\n",
    "- `seed = 123456`\n",
    "- `rng = np.random.default_rng(seed)`\n",
    "- `idx_feat = (np.floor(30*rng.uniform(size=4))).astype(np.int)`\n",
    "- `X = cancer[\"data\"][:,idx_feat]`\n",
    "\n",
    "**Again, you may use the entire set of examples in the dataset for training the model. There is no need to set aside a test set for this project.**\n",
    "\n",
    "6. **Outputs: State the seed value, the feature names, and the logistic regression score for the two best and two worst combinations of features found during your testing. Don't worry about finding the optimal solution. We are only interested in gaining some familiarity with the variation in the accuracty of the model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed values: [302173 316507 355899 153838 555542 143400 383041 571638 727986 560737]\n",
      "\n",
      "Two best combinations of features:\n",
      "                                            features     score    seed  \\\n",
      "1  [mean smoothness, concave points error, worst ...  0.926186  316507   \n",
      "7  [perimeter error, mean compactness, worst radi...  0.919156  571638   \n",
      "\n",
      "     feature index  \n",
      "1  [4, 17, 26, 23]  \n",
      "7  [12, 5, 20, 15]  \n",
      "\n",
      "Two worst combinations of features:\n",
      "                                            features     score    seed  \\\n",
      "5  [mean symmetry, radius error, perimeter error,...  0.799649  143400   \n",
      "4  [mean compactness, smoothness error, symmetry ...  0.690685  555542   \n",
      "\n",
      "     feature index  \n",
      "5  [8, 10, 12, 19]  \n",
      "4  [5, 14, 18, 18]  \n"
     ]
    }
   ],
   "source": [
    "# Load the breast cancer dataset\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "# Set the seed value\n",
    "seed = np.random.randint(100000, 999999, size=10)\n",
    "\n",
    "# Initialize an empty list to store the results\n",
    "results = []\n",
    "\n",
    "# Iterate over 10 combinations of features\n",
    "for i in range(10):\n",
    "    # Select 4 features randomly\n",
    "    rng = np.random.default_rng(seed[i])\n",
    "    idx_feat = (np.floor(30 * rng.uniform(size=4))).astype(int)\n",
    "    X = cancer.data[:, idx_feat]\n",
    "    y = cancer.target\n",
    "    \n",
    "    # Fit a logistic regression model on the selected features\n",
    "    model = LogisticRegression()\n",
    "\n",
    "    # Train the model using the training data\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Make predictions on the entire dataset\n",
    "    y_pred = model.predict(X)\n",
    "    \n",
    "    # Calculate the accuracy score\n",
    "    score = accuracy_score(y, y_pred)\n",
    "    \n",
    "    # Add the results to the list\n",
    "    results.append({'features': cancer.feature_names[idx_feat], 'score': score, 'seed': seed[i], 'feature index': idx_feat})\n",
    "\n",
    "# Convert the results to a Pandas DataFrame\n",
    "results = pd.DataFrame(results)\n",
    "\n",
    "# Sort the results by score in ascending order\n",
    "results = results.sort_values(by='score', ascending=False)\n",
    "\n",
    "# Print the seed value\n",
    "print(f\"Seed values: {seed}\")\n",
    "\n",
    "# Print the feature names and scores for the two best and two worst combinations\n",
    "print(\"\\nTwo best combinations of features:\")\n",
    "print(results.head(2)[['features', 'score', 'seed', 'feature index']])\n",
    "\n",
    "print(\"\\nTwo worst combinations of features:\")\n",
    "print(results.tail(2)[['features', 'score', 'seed', 'feature index']])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. **Questions: As you varied the \"seed\" value, what changes did you notice in the random selection of features? Were any of the features better in terms of increasing the logistic regression score? If you increased the number of features selected, would you get higher scores? What tradeoff would you be making if you selected more features? What other hyperparameters could you tune to improve the scores?**\n",
    "\n",
    "As the \"seed\" value changed, the random selection of features also changed, but the underlying dataset remains the same. No single feature was consistently better in terms of increasing the logistic regression score.\n",
    "\n",
    "Increasing the number of features selected may lead to higher scores, but this would result in the tradeoff of increased complexity and overfitting. Overfitting occurs when a model becomes too complex and memorizes the training data instead of learning the underlying patterns.\n",
    "\n",
    "To improve the scores, you could tune the hyperparameters such as the regularization strength, which controls the balance between fitting the data and avoiding overfitting, or the solver algorithm used to optimize the parameters of the model.\n",
    "\n",
    "8. **Python code that uses the Scikit-Learn Decision Tree Regressor algorithm to predict diabetes progression based on a subset of medical features. Similar to the Logistic Regression task above, use the pseudo-random number generator and \"seed\" to randomly select 4 features for building a decision tree. Use the entire set of examples in the dataset for training the model.**\n",
    "\n",
    "9. **Outputs: State the seed value, the feature names, the depth of the tree, the number of leaves of the tree and the decision tree coefficient of determination score for the two best and two worst combinations of features found during your testing. Don't worry about finding the optimal solution.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed value: [748845 474403 658597 315089 904593 318856 820800 878963 155233 622306]\n",
      "\n",
      "Two best combinations of features:\n",
      "             features  depth  leaves  score    seed feature index\n",
      "0  [age, s3, bmi, s1]     20     434    1.0  748845  [0, 6, 2, 4]\n",
      "1   [s5, s1, s6, age]     15     441    1.0  474403  [8, 4, 9, 0]\n",
      "\n",
      "Two worst combinations of features:\n",
      "            features  depth  leaves     score    seed feature index\n",
      "8  [s5, s1, bmi, s2]     19     435  1.000000  155233  [8, 4, 2, 5]\n",
      "9  [bp, age, s6, bp]     18     436  0.999993  622306  [3, 0, 9, 3]\n"
     ]
    }
   ],
   "source": [
    "# Load the diabetes dataset\n",
    "diabetes = load_diabetes()\n",
    "\n",
    "# Initialize an empty list to store the results\n",
    "results = []\n",
    "\n",
    "# Generate a list of 10 random seeds\n",
    "seed = np.random.randint(100000, 999999, size=10)\n",
    "\n",
    "# Loop through each seed\n",
    "for i in range(10):\n",
    "    rng = np.random.default_rng(seed[i])\n",
    "    # Select a subset of features\n",
    "    idx_feat = (np.floor(10*rng.uniform(size=4))).astype(int)\n",
    "    X = diabetes.data[:,idx_feat]\n",
    "    y = diabetes.target\n",
    "\n",
    "    # Create a DecisionTreeRegressor object\n",
    "    model = DecisionTreeRegressor()\n",
    "\n",
    "    # Fit the model on the training data\n",
    "    model.fit(X, y)\n",
    "\n",
    "    # Evaluate the model\n",
    "    score = model.score(X, y)\n",
    "    \n",
    "    # Get the depth and number of leaves of the tree\n",
    "    depth = model.tree_.max_depth\n",
    "    leaves = model.tree_.node_count + 1 - model.tree_.n_leaves\n",
    "\n",
    "    # Add the results to the list\n",
    "    results.append({'features': [diabetes.feature_names[i] for i in idx_feat], 'depth': depth, 'leaves': leaves, 'score': score, 'seed': seed[i], 'feature index': idx_feat})\n",
    "\n",
    "# Convert the results to a Pandas DataFrame\n",
    "results = pd.DataFrame(results)\n",
    "\n",
    "# Sort the results by score in ascending order\n",
    "results = results.sort_values(by='score', ascending=False)\n",
    "\n",
    "# Print the seed value\n",
    "print(f\"Seed value: {seed}\")\n",
    "\n",
    "# Print the feature names, depth, number of leaves and the decision tree coefficient of determination score\n",
    "# for the two best and two worst combinations of features\n",
    "print(\"\\nTwo best combinations of features:\")\n",
    "print(results.head(2)[['features', 'depth', 'leaves', 'score', 'seed', 'feature index']])\n",
    "\n",
    "print(\"\\nTwo worst combinations of features:\")\n",
    "print(results.tail(2)[['features', 'depth', 'leaves', 'score', 'seed', 'feature index']])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. **Questions: As you varied the \"seed\" value, what changes did you notice in the random selection of features? Were any of the features better in terms of increasing the decision tree score? If you increased the number of features selected, would you get higher scores? What tradeoff would you be making if you selected more features? What other hyperparameters could you tune to improve the scores?**\n",
    "\n",
    "Varying the \"seed\" value in the code would change the random selection of features. Different features may be selected, and hence the decision tree score could be different as well. However, increasing the number of features selected would not necessarily result in higher scores. In fact, selecting too many features may lead to overfitting, leading to poor generalization performance on unseen data. The tradeoff of selecting more features would be the risk of overfitting vs the potential benefit of better capturing the underlying relationship between the features and the target.\n",
    "\n",
    "To improve the scores, one could tune other hyperparameters such as the maximum depth of the tree, the minimum number of samples required to split an internal node, and the minimum number of samples required to be at a leaf node. These hyperparameters can be adjusted to control the complexity of the model and prevent overfitting.\n",
    "\n",
    "11. **Python code that uses the Scikit-Learn Support Vector Machine algorithm LinearSVC to predict the presence of cancer based on the processed medical image data in the breast cancer dataset. Follow the same procedure as was used for Logistic Regression.**\n",
    "\n",
    "12. **Outputs: State the seed value, the feature names, and the LinearSVC score for the two best and two worst combinations of features found during your testing. Don't worry about finding the optimal solution.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed value: [634638 782590 749754 467201 947057 478985 199746 995979 412313 468623]\n",
      "\n",
      "Two best seeds:\n",
      "                                            features     score    seed\n",
      "1  [mean concave points, mean compactness, mean t...  0.938489  782590\n",
      "6  [mean radius, mean compactness, mean smoothnes...  0.933216  199746\n",
      "\n",
      "Two worst seeds:\n",
      "                                            features     score    seed\n",
      "5  [mean fractal dimension, mean perimeter, mean ...  0.905097  478985\n",
      "0  [mean texture, mean texture, mean concavity, m...  0.887522  634638\n"
     ]
    }
   ],
   "source": [
    "# Load the breast cancer dataset\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "# Initialize an empty list to store the results\n",
    "results = []\n",
    "\n",
    "# Generate a list of 10 random seeds\n",
    "seed = np.random.randint(100000, 999999, size=10)\n",
    "\n",
    "# Loop through each seed\n",
    "for i in range(10):\n",
    "    rng = np.random.default_rng(seed[i])\n",
    "    idx_feat = (np.floor(10*rng.uniform(size=4))).astype(int)\n",
    "    X = cancer.data[:, idx_feat]\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    y = cancer.target\n",
    "\n",
    "    # Create a LinearSVC object\n",
    "    model = LinearSVC(max_iter=1500)\n",
    "\n",
    "    # Fit the model on the training data\n",
    "    model.fit(X, y)\n",
    "\n",
    "    # Calculate the accuracy score\n",
    "    score = model.score(X, y)\n",
    "\n",
    "    # Add the results to the list\n",
    "    #results.append({'accuracy': score, 'seed': seed[i]})\n",
    "\n",
    "    # Add the results to the list\n",
    "    results.append({'features': [cancer.feature_names[i] for i in idx_feat],\n",
    "                    'score': score, 'seed': seed[i], 'feature index': idx_feat})\n",
    "\n",
    "# Convert the results to a Pandas DataFrame\n",
    "results = pd.DataFrame(results)\n",
    "\n",
    "# Sort the results by accuracy in ascending order\n",
    "results = results.sort_values(by='score', ascending=False)\n",
    "\n",
    "# Print the seed value\n",
    "print(f\"Seed value: {seed}\")\n",
    "\n",
    "# Print the accuracy score for the two best and two worst seeds\n",
    "print(\"\\nTwo best seeds:\")\n",
    "print(results.head(2)[['features', 'score', 'seed']])\n",
    "\n",
    "print(\"\\nTwo worst seeds:\")\n",
    "print(results.tail(2)[['features', 'score', 'seed']])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. **Questions: As you varied the \"seed\" value, what changes did you notice in the random selection of features? Were any of the features better in terms of increasing the LinearSVC score? If you increased the number of features selected, would you get higher scores? What tradeoff would you be making if you selected more features? What other hyperparameters could you tune to improve the scores?**\n",
    "\n",
    "As the \"seed\" value changes, the random selection of features will change. This means that each time you run the code with a different seed value, you will get a different set of features selected. It is not possible to determine if any of the features are better in terms of increasing the LinearSVC score without running the model for every combination of features and for multiple values of the seed. This would require a lot of computation time.\n",
    "\n",
    "If you increase the number of features selected, you might get higher scores, but there is no guarantee. Selecting more features can increase the complexity of the model and lead to overfitting, where the model performs well on the training data but poorly on unseen data. In addition, by selecting more features, you would be making a tradeoff between model complexity and generalization performance. In general, a more complex model is more likely to overfit the data, while a simpler model is more likely to underfit.\n",
    "\n",
    "There are several hyperparameters that could be tuned to improve the scores, such as the regularization strength (applying a penalty to increasing the magnitude of parameter values in order to reduce overfitting) and the choice of the loss function. The regularization strength controls the tradeoff between accuracy and model complexity, while the choice of loss function determines the criteria used for optimization.\n",
    "\n",
    "To conclude, in the development of the code, problems arose as the model did not converge, to solve this, we opted to normalize the data and increse the number of iterations which seemed to solve the convergence error.\n",
    "\n",
    "14. **Python code that uses the Scikit-Learn KNeighborsRegressor algorithm to predict diabetes progression based on a subset of medical features. Follow the same procedure as was used for the Decision Tree Regressor.**\n",
    "\n",
    "15. **Outputs: State the seed value, the feature names, the number of neighbors and the KNeighborsRegressor coefficient of determination score for the two best and two worst combinations of features found during your testing. Don't worry about finding the optimal solution.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed value: [893914 833534 252137 827452 102802 314454 721234 801847 206437 970446]\n",
      "\n",
      "Two best combinations of features:\n",
      "            features     score    seed feature index  neighbors\n",
      "8  [bp, bmi, s6, s5]  0.576376  206437  [3, 2, 9, 8]          5\n",
      "1  [s2, bmi, s4, s6]  0.556789  833534  [5, 2, 7, 9]          5\n",
      "\n",
      "Two worst combinations of features:\n",
      "             features     score    seed feature index  neighbors\n",
      "7  [bmi, bmi, s2, s1]  0.481650  801847  [2, 2, 5, 4]          5\n",
      "6    [s5, bp, s2, bp]  0.457466  721234  [8, 3, 5, 3]          5\n"
     ]
    }
   ],
   "source": [
    "# Load the diabetes dataset\n",
    "diabetes = load_diabetes()\n",
    "\n",
    "# Initialize an empty list to store the results\n",
    "results = []\n",
    "\n",
    "# Generate a list of 10 random seeds\n",
    "seed = np.random.randint(100000, 999999, size=10)\n",
    "\n",
    "# Loop through each seed\n",
    "for i in range(10):\n",
    "    rng = np.random.default_rng(seed[i])\n",
    "    # Select a subset of features\n",
    "    idx_feat = (np.floor(10*rng.uniform(size=4))).astype(int)\n",
    "    X = diabetes.data[:, idx_feat]\n",
    "    y = diabetes.target\n",
    "\n",
    "    # Create a KNeighborsRegressor object\n",
    "    model = KNeighborsRegressor()\n",
    "\n",
    "    # Fit the model on the training data\n",
    "    model.fit(X, y)\n",
    "\n",
    "    # Evaluate the model\n",
    "    score = model.score(X, y)\n",
    "\n",
    "    # Add the results to the list\n",
    "    results.append({'features': [diabetes.feature_names[i] for i in idx_feat],\n",
    "                    'score': score, 'seed': seed[i], 'feature index': idx_feat, 'neighbors':np.size(model.kneighbors(return_distance=False),1)})\n",
    "\n",
    "# Convert the results to a Pandas DataFrame\n",
    "results = pd.DataFrame(results)\n",
    "\n",
    "# Sort the results by score in ascending order\n",
    "results = results.sort_values(by='score', ascending=False)\n",
    "\n",
    "# Print the seed value\n",
    "print(f\"Seed value: {seed}\")\n",
    "\n",
    "# Print the feature names, depth, number of leaves and the decision tree coefficient of determination score\n",
    "# for the two best and two worst combinations of features\n",
    "print(\"\\nTwo best combinations of features:\")\n",
    "print(results.head(2)[['features', 'score', 'seed', 'feature index', 'neighbors']])\n",
    "\n",
    "print(\"\\nTwo worst combinations of features:\")\n",
    "print(results.tail(2)[['features', 'score', 'seed', 'feature index', 'neighbors']])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16. **Questions: As you varied the \"seed\" value, what changes did you notice in the random selection of features? Were any of the features better in terms of increasing the k-Nearest Neighbors score? If you increased the number of features selected, would you get higher scores? What tradeoff would you be making if you selected more features? What other hyperparameters could you tune to improve the scores?**\n",
    "\n",
    "As you vary the \"seed\" value, the random selection of features will change. This is because the random number generator is being initialized with a different seed each time, resulting in a different sequence of random numbers being generated.\n",
    "\n",
    "In terms of increasing the k-Nearest Neighbors score, it is not possible to determine which features are better based on a single random selection of features. It would be necessary to run multiple trials with different seed values and different combinations of features to determine which combinations of features result in higher scores.\n",
    "\n",
    "Increasing the number of features selected may result in higher scores, but this comes with a tradeoff. Selecting more features can lead to overfitting, where the model becomes too complex and starts to memorize the training data instead of generalizing to new data.\n",
    "\n",
    "There are a number of other hyperparameters that can be tuned to improve the scores, including the number of neighbors (k), the distance metric used to calculate the distances between examples, and the weighting scheme used to assign weights to the neighbors in making predictions.\n",
    "\n",
    "17. **A brief conclusion for the project.**\n",
    "\n",
    "In this project, we explored several of the fundamental supervised learning algorithms, including Linear Regression, Logistic Regression, Decision Trees, Support Vector Machines, and k-Nearest Neighbors. Through this exploration, we learned about the strengths and weaknesses of each of these algorithms and how they can be used to make predictions based on a set of input features.\n",
    "\n",
    "In terms of future assignments, this knowledge will be incredibly useful. Many data science projects require the use of machine learning algorithms, and having a solid understanding of these algorithms will allow us to select the most appropriate one for the task at hand. Additionally, having hands-on experience with these algorithms will allow us to implement them quickly and effectively in future projects.\n",
    "\n",
    "Overall, this project was a valuable learning experience that has given us a deeper understanding of supervised learning algorithms. With this knowledge, we are well-equipped to tackle more complex data science problems in the future."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ac2eaa0ea0ebeafcc7822e65e46aa9d4f966f30b695406963e145ea4a91cd4fc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

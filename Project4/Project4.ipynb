{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***ECE563: AI in Smart Grid***\n",
    "\n",
    "Arturo Galofr√© (A20521022)\n",
    "\n",
    "1. **A brief overview of the project.**\n",
    "\n",
    "This project involved building and evaluating machine learning models using the scikit-learn library in Python. The goal was to develop classifiers and regressors for both the Diabetes and the Cancer Scikit datasets. The datasets were split into training, validation, and test subsets, and various algorithms, including Random Forest and Gradient Boosting, were utilized to train and validate models. Hyperparameters were tuned to optimize model performance, and the models were evaluated based on their scores on the training, validation, and test datasets.\n",
    "\n",
    "The design process involved considering various tradeoffs, such as model complexity, interpretability, and performance. Multiple iterations were performed to fine-tune the models and improve their prediction accuracy on the validation and test data. Python code was used to implement the models, and hyperparameters were adjusted from their default values to optimize model performance.\n",
    "\n",
    "\n",
    "2. **Python code that splits the original Wisconsin breast cancer dataset into two subsets: training/validation (80%), and test (20%). Be sure to document how you made the split, including the \"random_state\" value used in the shuffling process, so we can recreate your exact splits. See \"model_selection.train_test_split\" for guidance.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (341, 30)\n",
      "Validation set shape: (114, 30)\n",
      "Test set shape: (114, 30)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the data into training/validation and test sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=3)\n",
    "\n",
    "# Further split the training/validation set into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=3)\n",
    "\n",
    "# Print the shapes of the resulting datasets\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Validation set shape:\", X_val.shape)\n",
    "print(\"Test set shape:\", X_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Python code that uses an additional split to create a validation dataset or Python code that implements a cross-validation approach to tune the Random Forest model hyperparameters. Be sure to document how you created the validation data, including the \"random_state\" value used in the shuffling process, so we can recreate your exact splits. See \"model_selection.train_test_split\" or Scikit-Learn's User Guide Section 3 (Model selection and evaluation) for guidance.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters:  {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Best Accuracy:  0.9707587382779199\n",
      "Test Accuracy:  0.9385964912280702\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "# Load the breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=3)\n",
    "\n",
    "# Further split the training set into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=3)\n",
    "\n",
    "# Create a Random Forest classifier\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Define the hyperparameter grid for tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 50, 100],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Perform Grid Search Cross Validation to find the best hyperparameters\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters and the corresponding accuracy score\n",
    "print(\"Best Hyperparameters: \", grid_search.best_params_)\n",
    "print(\"Best Accuracy: \", grid_search.best_score_)\n",
    "\n",
    "# Evaluate the model on the test set using the best hyperparameters\n",
    "best_rf = grid_search.best_estimator_\n",
    "test_accuracy = best_rf.score(X_test, y_test)\n",
    "print(\"Test Accuracy: \", test_accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Procedure documenting your design process and the tradeoffs you considered in building a Random Forest Classifier.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When designing a Random Forest classifier, there are several key steps to consider:\n",
    "\n",
    "- **Data preparation**: This step involves loading and preprocessing the dataset. It may include tasks such as handling missing values, normalizing or scaling features, and encoding categorical variables. It's important to carefully preprocess the data to ensure that it's in a format suitable for training a Random Forest model.\n",
    "- **Feature selection**: Random Forests are capable of handling a large number of features, but including irrelevant or redundant features can lead to overfitting and decreased performance. Therefore, it's important to carefully select relevant features that are likely to contribute to the predictive accuracy of the model.\n",
    "- **Model configuration**: Random Forests have several hyperparameters that need to be configured, such as the number of trees in the forest (n_estimators), the maximum depth of the trees (max_depth), the minimum number of samples required to split an internal node (min_samples_split), and the minimum number of samples required to be at a leaf node (min_samples_leaf). Carefully tuning these hyperparameters can greatly impact the performance of the model.\n",
    "- **Model training**: Random Forests are an ensemble technique that combines the predictions of multiple decision trees to make a final prediction. The training process involves fitting multiple decision trees to the training data, where each tree is trained on a random subset of features and samples. This randomness helps to reduce overfitting and improve the generalization performance of the model.\n",
    "- **Model evaluation**: After training the Random Forest model, it's important to evaluate its performance on a separate validation or test set. Common evaluation metrics for classification tasks include accuracy, precision, recall, F1-score, and area under the receiver operating characteristic (ROC) curve. It's important to choose appropriate evaluation metrics based on the specific problem domain and requirements.\n",
    "\n",
    "Tradeoffs to consider when building a Random Forest classifier include:\n",
    "\n",
    "- **Model complexity vs. performance**: Increasing the number of trees in the forest (n_estimators) or the maximum depth of the trees (max_depth) can increase the complexity of the model, potentially leading to overfitting. Finding the right balance between model complexity and performance is important to achieve optimal results.\n",
    "- **Computational efficiency**: Random Forests can be computationally expensive, especially when dealing with large datasets or a large number of features. Training a large number of trees or using complex feature selection methods can increase the training time of the model. It's important to consider the computational resources available and choose appropriate hyperparameter values accordingly.\n",
    "- **Interpretability vs. predictive accuracy**: Random Forests are often referred to as \"black box\" models because the individual decision trees are not easily interpretable. If interpretability is a priority, other models like decision trees or logistic regression may be more suitable. However, Random Forests are known for their high predictive accuracy and can be a good choice when interpretability is not a primary concern."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **Python code that uses RandomForestClassifier to train, validate and test a Random Forest model. You may use any number of features from the dataset. Be sure to set the \"random_state\" so we can recreate your model.**\n",
    "6. **Inputs: A list of hyperparameters, and their new values, that were modified from their default values**\n",
    "7. **Outputs: The score value of the final model for each of the datasets: training, validation and test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 1.0000\n",
      "Validation set accuracy: 0.9912\n",
      "Test set accuracy: 0.9298\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "\n",
    "# Load the Wisconsin breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the data into training/validation (80%) and test (20%) sets\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, test_size=0.2, random_state=3)\n",
    "\n",
    "# Split the training/validation set further into training (60%) and validation (20%) sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=0.25, random_state=3)\n",
    "\n",
    "# Define the hyperparameter values to be modified\n",
    "n_estimators = 50\n",
    "max_depth = 20\n",
    "min_samples_split = 2\n",
    "min_samples_leaf = 1\n",
    "\n",
    "# Initialize and train the Random Forest classifier\n",
    "rf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf, random_state=3)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the training set\n",
    "y_train_pred = rf.predict(X_train)\n",
    "train_score = accuracy_score(y_train, y_train_pred)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_val_pred = rf.predict(X_val)\n",
    "val_score = accuracy_score(y_val, y_val_pred)\n",
    "\n",
    "# Predict on the test set\n",
    "y_test_pred = rf.predict(X_test)\n",
    "test_score = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "# Print the accuracy scores for the training, validation, and test sets\n",
    "print(\"Training set accuracy: {:.4f}\".format(train_score))\n",
    "print(\"Validation set accuracy: {:.4f}\".format(val_score))\n",
    "print(\"Test set accuracy: {:.4f}\".format(test_score))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. **Observations: What could you do to improve the prediction score of your trained model on the validation data? How well does your final model predict the classes in the test data? Provide a list of all of the examples from the test data that are predicted incorrectly.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified examples in the test set, 8 out of 114:\n",
      "Example 1: [14.26 19.65 97.83]...\n",
      "Example 2: [ 17.85  13.23 114.6 ]...\n",
      "Example 3: [13.34 15.86 86.49]...\n",
      "Example 4: [ 15.37  22.76 100.2 ]...\n",
      "Example 5: [13.8  15.79 90.43]...\n",
      "Example 6: [ 16.84  19.46 108.4 ]...\n",
      "Example 7: [13.44 21.58 86.18]...\n",
      "Example 8: [14.6  23.29 93.97]...\n"
     ]
    }
   ],
   "source": [
    "# Find misclassified examples in the test set\n",
    "misclassified_idx = y_test != y_test_pred\n",
    "misclassified_examples = X_test[misclassified_idx]\n",
    "\n",
    "# Print misclassified examples\n",
    "print(\"Misclassified examples in the test set, {} out of {}:\".format(len(misclassified_examples), len(y_test)))\n",
    "for i, example in enumerate(misclassified_examples):\n",
    "    print(\"Example {}: {}...\".format(i+1, example[0:3]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from the values obtained from the test data, the acuracy score is 0.9298 which coul dbe considered to be a high value, which means the algoriths is doing a good job at predicting the class from the test data. To improve the prediction score of the trained model on the validation data, we can try the following approaches:\n",
    "\n",
    "- **Hyperparameter tuning**: Experiment with different values of hyperparameters such as n_estimators, max_depth, min_samples_split, and min_samples_leaf to find the optimal combination that results in better model performance on the validation data. This can be done using techniques such as grid search or randomized search.\n",
    "- **Feature engineering**: Analyze the features in the dataset and explore different feature engineering techniques such as feature scaling, feature selection, or feature extraction to improve the predictive power of the model.\n",
    "- **Model ensemble**: Consider using ensemble methods such as bagging or boosting in combination with the Random Forest model to potentially improve the model's performance.\n",
    "- **Data augmentation**: If the dataset is small, consider using techniques such as data augmentation to generate synthetic samples and increase the size of the training data, which can help the model to learn better representations.\n",
    "- **Cross-validation**: Use cross-validation to get a more reliable estimate of the model's performance by training and evaluating the model on multiple subsets of the data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. **The above process should be repeated with the Gradient Boosting learning algorithm. You should reuse the same splits by using the same random_state values. This way, your Gradient Boosting Classifier will see the same training, validation and test data. When you have completed the process with the Gradient Boosting Classifier, compare and contrast the results with those from the Random Forest Classifier. Do both algorithms make the same classification mistakes on the test data? Is one clearly better than the other? What advantages and disadvantages did you find for each of the two algorithms?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters:  {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200}\n",
      "Best Cross-validation Score:  0.9698249619482496\n",
      "Test Set Score:  0.9385964912280702\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the data into training/validation (80%) and test (20%) sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=3)\n",
    "\n",
    "# Split the training/validation set into training (80%) and validation (20%) sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=3)\n",
    "\n",
    "# Create a Gradient Boosting model\n",
    "gbm = GradientBoostingClassifier()\n",
    "\n",
    "# Define hyperparameters and their possible values to tune\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5]\n",
    "}\n",
    "\n",
    "# Perform grid search cross-validation\n",
    "grid_search = GridSearchCV(estimator=gbm, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters and corresponding score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "# Train the final model with the best hyperparameters on the entire training/validation set\n",
    "gbm_best = GradientBoostingClassifier(**best_params)\n",
    "gbm_best.fit(X_train_val, y_train_val)\n",
    "\n",
    "# Evaluate the final model on the test set\n",
    "test_score = gbm_best.score(X_test, y_test)\n",
    "\n",
    "print(\"Best Hyperparameters: \", best_params)\n",
    "print(\"Best Cross-validation Score: \", best_score)\n",
    "print(\"Test Set Score: \", test_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 1.0000\n",
      "Validation Score: 0.9890\n",
      "Test Score: 0.9386\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the data into training/validation (80%) and test (20%) sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=3)\n",
    "\n",
    "# Further split the training/validation set into training (80%) and validation (20%) sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=3)\n",
    "\n",
    "# Create a Gradient Boosting Classifier with modified hyperparameters\n",
    "gb_classifier = GradientBoostingClassifier(\n",
    "    n_estimators=200,  # number of boosting stages\n",
    "    learning_rate=0.1,  # learning rate\n",
    "    max_depth=3,  # maximum depth of individual regression estimators\n",
    "    min_samples_split=2,  # minimum number of samples required to split an internal node\n",
    "    min_samples_leaf=1,  # minimum number of samples required to be at a leaf node\n",
    "    subsample=1.0,  # fraction of samples used for fitting the individual base learners\n",
    "    random_state=42  # random state for reproducibility\n",
    ")\n",
    "\n",
    "# Train the Gradient Boosting Classifier on the training data\n",
    "gb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the training data\n",
    "y_train_pred = gb_classifier.predict(X_train)\n",
    "train_score = accuracy_score(y_train, y_train_pred)\n",
    "\n",
    "# Evaluate the model on the validation data\n",
    "y_val_pred = gb_classifier.predict(X_val)\n",
    "val_score = accuracy_score(y_val, y_val_pred)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "y_test_pred = gb_classifier.predict(X_test)\n",
    "test_score = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "# Print the scores\n",
    "print(\"Training Score: {:.4f}\".format(train_score))\n",
    "print(\"Validation Score: {:.4f}\".format(val_score))\n",
    "print(\"Test Score: {:.4f}\".format(test_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified examples in the test set, 7 out of 114:\n",
      "Example 1: [ 17.85  13.23 114.6 ]...\n",
      "Example 2: [13.34 15.86 86.49]...\n",
      "Example 3: [ 15.37  22.76 100.2 ]...\n",
      "Example 4: [13.8  15.79 90.43]...\n",
      "Example 5: [ 16.84  19.46 108.4 ]...\n",
      "Example 6: [13.44 21.58 86.18]...\n",
      "Example 7: [14.6  23.29 93.97]...\n"
     ]
    }
   ],
   "source": [
    "# Find misclassified examples in the test set\n",
    "misclassified_idx = y_test != y_test_pred\n",
    "misclassified_examples = X_test[misclassified_idx]\n",
    "\n",
    "# Print misclassified examples\n",
    "print(\"Misclassified examples in the test set, {} out of {}:\".format(len(misclassified_examples), len(y_test)))\n",
    "for i, example in enumerate(misclassified_examples):\n",
    "    print(\"Example {}: {}...\".format(i+1, example[0:3]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completing the process with both the Random Forest Classifier and the Gradient Boosting Classifier, it's important to compare and contrast the results to understand their performance and trade-offs.\n",
    "\n",
    "It's possible that both algorithms may make similar classification mistakes on the test data, as they are both ensemble learning algorithms and may have similar biases or limitations. However, it's also possible that they may make different mistakes, as they have different underlying mechanisms for building and optimizing the ensemble of base models.\n",
    "\n",
    "To compare the performance of the two algorithms, it's important to analyze the scores obtained on the test data for each algorithm. The algorithm with a higher score is generally considered to be performing better. In our case, the Random Forest Classifier obtained a score of 0.9298 on the test data while the Gradient Boosting Classifier got a value of 0.9386. By these values we would consider the Gradient Boosting Classifier to be the most accurate of the two classifiers considered though both obtained silimar high scores in the testing.\n",
    "\n",
    "**Advantages and disadvantages**\n",
    "1. Random Forest Classifier (RFC):\n",
    "- Advantages:\n",
    "The Random Forest Classifier is typically faster to train compared to Gradient Boosting Classifier, as it builds each tree independently and in parallel.\n",
    "Moreover this model is less prone to overfitting due to the averaging effect of multiple trees. Also, it has the ability to handle categorical and numerical features without the need for extensive data preprocessing.\n",
    "- Disadvantages:\n",
    "The RFC may not always achieve the highest prediction accuracy compared to other ensemble algorithms like Gradient Boosting, as the individual trees are not optimized for the errors of other trees. In addition, it may have limitations in handling imbalanced datasets or datasets with high-dimensional features.\n",
    "2. Gradient Boosting Classifier (GBC):\n",
    "- Advantages:\n",
    "The GBC Can often achieve higher prediction accuracy compared to Random Forest Classifier. Also, it can handle imbalanced datasets better by using techniques such as weighted sampling or cost-sensitive learning. And lastly, it captures complex interactions between features and potentially perform well on high-dimensional datasets.\n",
    "- Disadvantages:\n",
    "It may be slower to train compared to Random Forest Classifier. Moerover, is more prone to overfitting if the number of trees or the learning rate is set too high. The GBC may require careful tuning of hyperparameters to achieve optimal performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. **Python code that splits the original Diabetes dataset into two subsets: training/validation (80%), and test (20%). Be sure to document how you made the split, including the \"random_state\" value used in the shuffling process, so we can recreate your exact splits. See \"model_selection.train_test_split\" for guidance.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 282\n",
      "Validation set size: 71\n",
      "Test set size: 89\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "# Load the Diabetes dataset\n",
    "data = load_diabetes()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the dataset into training/validation and test sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=3)\n",
    "\n",
    "# Split the training/validation set into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=3)\n",
    "\n",
    "# Print the sizes of the resulting datasets\n",
    "print(\"Training set size:\", X_train.shape[0])\n",
    "print(\"Validation set size:\", X_val.shape[0])\n",
    "print(\"Test set size:\", X_test.shape[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. **Python code that uses an additional split to create a validation dataset or Python code that implements a cross-validation approach to tune the Random Forest model hyperparameters. Be sure to document how you created the validation data, including the \"random_state\" value used in the shuffling process, so we can recreate your exact splits. See \"model_selection.train_test_split\" or Scikit-Learn's User Guide Section 3 (Model selection and evaluation) for guidance.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 1077.8766633138307\n",
      "Validation MSE: 998.2657610969202\n",
      "Test MSE: 3185.5557866660693\n",
      "Best Hyperparameters:  {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 300}\n",
      "Best Cross-validation Score:  0.42799192992251134\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "# Load the Diabetes dataset\n",
    "data = load_diabetes()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the data into training/validation (80%) and test (20%) sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=3)\n",
    "\n",
    "# Use an additional split to create a validation dataset (20% of the training/validation set)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=3)\n",
    "\n",
    "# Define the hyperparameters to tune and their possible values\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 5, 10, 15],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Create the Random Forest Regressor\n",
    "rf = RandomForestRegressor(random_state=3)\n",
    "\n",
    "# Use GridSearchCV for hyperparameter tuning with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(rf, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Train the final model using the best hyperparameters on the combined training/validation set\n",
    "best_rf = RandomForestRegressor(random_state=3, **best_params)\n",
    "best_rf.fit(X_train_val, y_train_val)\n",
    "\n",
    "# Predict the targets on the training, validation, and test sets\n",
    "y_train_pred = best_rf.predict(X_train)\n",
    "y_val_pred = best_rf.predict(X_val)\n",
    "y_test_pred = best_rf.predict(X_test)\n",
    "\n",
    "# Calculate regression evaluation metrics\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "val_mse = mean_squared_error(y_val, y_val_pred)\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "# Print the regression evaluation metrics\n",
    "print('Training MSE:', train_mse)\n",
    "print('Validation MSE:', val_mse)\n",
    "print('Test MSE:', test_mse)\n",
    "\n",
    "# Get the best hyperparameters and corresponding score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "print(\"Best Hyperparameters: \", best_params)\n",
    "print(\"Best Cross-validation Score: \", best_score)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. **Procedure documenting your design process and the tradeoffs you considered in building a Random Forest Regressor.**\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The design process for building a Random Forest Regressor involves several steps and considerations:\n",
    "\n",
    "- **Data Preparation**: This step involves loading and preprocessing the dataset. It may include tasks such as handling missing values, encoding categorical variables, and normalizing or scaling numeric features. The Random Forest Regressor in Scikit-Learn requires numeric input features, so any necessary data transformations should be applied accordingly.\n",
    "- **Feature Selection**: It's important to select relevant features for the regression task. This can be done through various techniques such as domain knowledge, feature importance analysis, or feature selection algorithms. Selecting the right set of features can greatly impact the performance of the Random Forest Regressor.\n",
    "- **Hyperparameter Tuning**: The Random Forest Regressor has several hyperparameters that need to be tuned for optimal performance. These may include the number of trees in the forest, the maximum depth of the trees, the minimum number of samples required to split a node, and others. Careful experimentation with different hyperparameter values and their impact on model performance is necessary.\n",
    "- **Model Evaluation**: It's important to evaluate the performance of the Random Forest Regressor using appropriate evaluation metrics such as mean squared error (MSE), root mean squared error (RMSE), or R-squared (R2) score. This allows us to assess the accuracy and generalization capability of the model.\n",
    "- **Overfitting Prevention**: Random Forest models are prone to overfitting, especially when the number of trees in the forest is high. Techniques such as limiting the depth of the trees, setting a minimum number of samples required to split a node, and setting a maximum number of features considered for splitting can help prevent overfitting.\n",
    "- **Model Interpretability**: Random Forest models are generally considered as black-box models, which means they may lack interpretability. However, some techniques such as feature importance analysis can provide insights into which features are most influential in making predictions.\n",
    "- **Tradeoffs**: Some tradeoffs in building a Random Forest Regressor include the potential for overfitting if the number of trees in the forest is too high, the need for careful hyperparameter tuning to optimize performance, and the lack of interpretability compared to simpler models like linear regression."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. **Python code that uses RandomForestRegressor to train, validate and test a Random Forest model. You may use any number of features from the dataset. Be sure to set the \"random_state\" so we can recreate your model.**\n",
    "14. **Inputs: A list of hyperparameters, and their new values, that were modified from their default values**\n",
    "15. **Outputs: The score value of the final model for each of the datasets: training, validation and test**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': array([[ 0.03807591,  0.05068012,  0.06169621, ..., -0.00259226,\n",
      "         0.01990842, -0.01764613],\n",
      "       [-0.00188202, -0.04464164, -0.05147406, ..., -0.03949338,\n",
      "        -0.06832974, -0.09220405],\n",
      "       [ 0.08529891,  0.05068012,  0.04445121, ..., -0.00259226,\n",
      "         0.00286377, -0.02593034],\n",
      "       ...,\n",
      "       [ 0.04170844,  0.05068012, -0.01590626, ..., -0.01107952,\n",
      "        -0.04687948,  0.01549073],\n",
      "       [-0.04547248, -0.04464164,  0.03906215, ...,  0.02655962,\n",
      "         0.04452837, -0.02593034],\n",
      "       [-0.04547248, -0.04464164, -0.0730303 , ..., -0.03949338,\n",
      "        -0.00421986,  0.00306441]]), 'target': array([151.,  75., 141., 206., 135.,  97., 138.,  63., 110., 310., 101.,\n",
      "        69., 179., 185., 118., 171., 166., 144.,  97., 168.,  68.,  49.,\n",
      "        68., 245., 184., 202., 137.,  85., 131., 283., 129.,  59., 341.,\n",
      "        87.,  65., 102., 265., 276., 252.,  90., 100.,  55.,  61.,  92.,\n",
      "       259.,  53., 190., 142.,  75., 142., 155., 225.,  59., 104., 182.,\n",
      "       128.,  52.,  37., 170., 170.,  61., 144.,  52., 128.,  71., 163.,\n",
      "       150.,  97., 160., 178.,  48., 270., 202., 111.,  85.,  42., 170.,\n",
      "       200., 252., 113., 143.,  51.,  52., 210.,  65., 141.,  55., 134.,\n",
      "        42., 111.,  98., 164.,  48.,  96.,  90., 162., 150., 279.,  92.,\n",
      "        83., 128., 102., 302., 198.,  95.,  53., 134., 144., 232.,  81.,\n",
      "       104.,  59., 246., 297., 258., 229., 275., 281., 179., 200., 200.,\n",
      "       173., 180.,  84., 121., 161.,  99., 109., 115., 268., 274., 158.,\n",
      "       107.,  83., 103., 272.,  85., 280., 336., 281., 118., 317., 235.,\n",
      "        60., 174., 259., 178., 128.,  96., 126., 288.,  88., 292.,  71.,\n",
      "       197., 186.,  25.,  84.,  96., 195.,  53., 217., 172., 131., 214.,\n",
      "        59.,  70., 220., 268., 152.,  47.,  74., 295., 101., 151., 127.,\n",
      "       237., 225.,  81., 151., 107.,  64., 138., 185., 265., 101., 137.,\n",
      "       143., 141.,  79., 292., 178.,  91., 116.,  86., 122.,  72., 129.,\n",
      "       142.,  90., 158.,  39., 196., 222., 277.,  99., 196., 202., 155.,\n",
      "        77., 191.,  70.,  73.,  49.,  65., 263., 248., 296., 214., 185.,\n",
      "        78.,  93., 252., 150.,  77., 208.,  77., 108., 160.,  53., 220.,\n",
      "       154., 259.,  90., 246., 124.,  67.,  72., 257., 262., 275., 177.,\n",
      "        71.,  47., 187., 125.,  78.,  51., 258., 215., 303., 243.,  91.,\n",
      "       150., 310., 153., 346.,  63.,  89.,  50.,  39., 103., 308., 116.,\n",
      "       145.,  74.,  45., 115., 264.,  87., 202., 127., 182., 241.,  66.,\n",
      "        94., 283.,  64., 102., 200., 265.,  94., 230., 181., 156., 233.,\n",
      "        60., 219.,  80.,  68., 332., 248.,  84., 200.,  55.,  85.,  89.,\n",
      "        31., 129.,  83., 275.,  65., 198., 236., 253., 124.,  44., 172.,\n",
      "       114., 142., 109., 180., 144., 163., 147.,  97., 220., 190., 109.,\n",
      "       191., 122., 230., 242., 248., 249., 192., 131., 237.,  78., 135.,\n",
      "       244., 199., 270., 164.,  72.,  96., 306.,  91., 214.,  95., 216.,\n",
      "       263., 178., 113., 200., 139., 139.,  88., 148.,  88., 243.,  71.,\n",
      "        77., 109., 272.,  60.,  54., 221.,  90., 311., 281., 182., 321.,\n",
      "        58., 262., 206., 233., 242., 123., 167.,  63., 197.,  71., 168.,\n",
      "       140., 217., 121., 235., 245.,  40.,  52., 104., 132.,  88.,  69.,\n",
      "       219.,  72., 201., 110.,  51., 277.,  63., 118.,  69., 273., 258.,\n",
      "        43., 198., 242., 232., 175.,  93., 168., 275., 293., 281.,  72.,\n",
      "       140., 189., 181., 209., 136., 261., 113., 131., 174., 257.,  55.,\n",
      "        84.,  42., 146., 212., 233.,  91., 111., 152., 120.,  67., 310.,\n",
      "        94., 183.,  66., 173.,  72.,  49.,  64.,  48., 178., 104., 132.,\n",
      "       220.,  57.]), 'frame': None, 'DESCR': '.. _diabetes_dataset:\\n\\nDiabetes dataset\\n----------------\\n\\nTen baseline variables, age, sex, body mass index, average blood\\npressure, and six blood serum measurements were obtained for each of n =\\n442 diabetes patients, as well as the response of interest, a\\nquantitative measure of disease progression one year after baseline.\\n\\n**Data Set Characteristics:**\\n\\n  :Number of Instances: 442\\n\\n  :Number of Attributes: First 10 columns are numeric predictive values\\n\\n  :Target: Column 11 is a quantitative measure of disease progression one year after baseline\\n\\n  :Attribute Information:\\n      - age     age in years\\n      - sex\\n      - bmi     body mass index\\n      - bp      average blood pressure\\n      - s1      tc, T-Cells (a type of white blood cells)\\n      - s2      ldl, low-density lipoproteins\\n      - s3      hdl, high-density lipoproteins\\n      - s4      tch, thyroid stimulating hormone\\n      - s5      ltg, lamotrigine\\n      - s6      glu, blood sugar level\\n\\nNote: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times `n_samples` (i.e. the sum of squares of each column totals 1).\\n\\nSource URL:\\nhttps://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\\n\\nFor more information see:\\nBradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\\n(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)', 'feature_names': ['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6'], 'data_filename': '/usr/local/lib/python3.9/site-packages/sklearn/datasets/data/diabetes_data.csv.gz', 'target_filename': '/usr/local/lib/python3.9/site-packages/sklearn/datasets/data/diabetes_target.csv.gz'}\n",
      "[[ 0.03807591  0.05068012  0.06169621 ... -0.00259226  0.01990842\n",
      "  -0.01764613]\n",
      " [-0.00188202 -0.04464164 -0.05147406 ... -0.03949338 -0.06832974\n",
      "  -0.09220405]\n",
      " [ 0.08529891  0.05068012  0.04445121 ... -0.00259226  0.00286377\n",
      "  -0.02593034]\n",
      " ...\n",
      " [ 0.04170844  0.05068012 -0.01590626 ... -0.01107952 -0.04687948\n",
      "   0.01549073]\n",
      " [-0.04547248 -0.04464164  0.03906215 ...  0.02655962  0.04452837\n",
      "  -0.02593034]\n",
      " [-0.04547248 -0.04464164 -0.0730303  ... -0.03949338 -0.00421986\n",
      "   0.00306441]]\n",
      "[151.  75. 141. 206. 135.  97. 138.  63. 110. 310. 101.  69. 179. 185.\n",
      " 118. 171. 166. 144.  97. 168.  68.  49.  68. 245. 184. 202. 137.  85.\n",
      " 131. 283. 129.  59. 341.  87.  65. 102. 265. 276. 252.  90. 100.  55.\n",
      "  61.  92. 259.  53. 190. 142.  75. 142. 155. 225.  59. 104. 182. 128.\n",
      "  52.  37. 170. 170.  61. 144.  52. 128.  71. 163. 150.  97. 160. 178.\n",
      "  48. 270. 202. 111.  85.  42. 170. 200. 252. 113. 143.  51.  52. 210.\n",
      "  65. 141.  55. 134.  42. 111.  98. 164.  48.  96.  90. 162. 150. 279.\n",
      "  92.  83. 128. 102. 302. 198.  95.  53. 134. 144. 232.  81. 104.  59.\n",
      " 246. 297. 258. 229. 275. 281. 179. 200. 200. 173. 180.  84. 121. 161.\n",
      "  99. 109. 115. 268. 274. 158. 107.  83. 103. 272.  85. 280. 336. 281.\n",
      " 118. 317. 235.  60. 174. 259. 178. 128.  96. 126. 288.  88. 292.  71.\n",
      " 197. 186.  25.  84.  96. 195.  53. 217. 172. 131. 214.  59.  70. 220.\n",
      " 268. 152.  47.  74. 295. 101. 151. 127. 237. 225.  81. 151. 107.  64.\n",
      " 138. 185. 265. 101. 137. 143. 141.  79. 292. 178.  91. 116.  86. 122.\n",
      "  72. 129. 142.  90. 158.  39. 196. 222. 277.  99. 196. 202. 155.  77.\n",
      " 191.  70.  73.  49.  65. 263. 248. 296. 214. 185.  78.  93. 252. 150.\n",
      "  77. 208.  77. 108. 160.  53. 220. 154. 259.  90. 246. 124.  67.  72.\n",
      " 257. 262. 275. 177.  71.  47. 187. 125.  78.  51. 258. 215. 303. 243.\n",
      "  91. 150. 310. 153. 346.  63.  89.  50.  39. 103. 308. 116. 145.  74.\n",
      "  45. 115. 264.  87. 202. 127. 182. 241.  66.  94. 283.  64. 102. 200.\n",
      " 265.  94. 230. 181. 156. 233.  60. 219.  80.  68. 332. 248.  84. 200.\n",
      "  55.  85.  89.  31. 129.  83. 275.  65. 198. 236. 253. 124.  44. 172.\n",
      " 114. 142. 109. 180. 144. 163. 147.  97. 220. 190. 109. 191. 122. 230.\n",
      " 242. 248. 249. 192. 131. 237.  78. 135. 244. 199. 270. 164.  72.  96.\n",
      " 306.  91. 214.  95. 216. 263. 178. 113. 200. 139. 139.  88. 148.  88.\n",
      " 243.  71.  77. 109. 272.  60.  54. 221.  90. 311. 281. 182. 321.  58.\n",
      " 262. 206. 233. 242. 123. 167.  63. 197.  71. 168. 140. 217. 121. 235.\n",
      " 245.  40.  52. 104. 132.  88.  69. 219.  72. 201. 110.  51. 277.  63.\n",
      " 118.  69. 273. 258.  43. 198. 242. 232. 175.  93. 168. 275. 293. 281.\n",
      "  72. 140. 189. 181. 209. 136. 261. 113. 131. 174. 257.  55.  84.  42.\n",
      " 146. 212. 233.  91. 111. 152. 120.  67. 310.  94. 183.  66. 173.  72.\n",
      "  49.  64.  48. 178. 104. 132. 220.  57.]\n",
      "Training score: 0.82\n",
      "Validation score: 0.53\n",
      "Test score: 0.41\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "# Load the Diabetes dataset\n",
    "data = load_diabetes()\n",
    "print(data)\n",
    "X = data.data\n",
    "print(X)\n",
    "\n",
    "y = data.target\n",
    "print(y)\n",
    "\n",
    "# Split the data into training/validation and test sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=3)\n",
    "\n",
    "# Further split the training/validation data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=3)\n",
    "\n",
    "# Instantiate the Random Forest Regressor with specified hyperparameters\n",
    "rf = RandomForestRegressor(n_estimators=300, max_depth=10, min_samples_leaf=1, min_samples_split=10, random_state=3)\n",
    "\n",
    "# Train the model on the training data\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Validate the model on the validation data\n",
    "y_val_pred = rf.predict(X_val)\n",
    "val_score = r2_score(y_val, y_val_pred)\n",
    "\n",
    "# Test the model on the test data\n",
    "y_test_pred = rf.predict(X_test)\n",
    "test_score = r2_score(y_test, y_test_pred)\n",
    "\n",
    "# Print the scores for training, validation, and test data\n",
    "print(\"Training score: {:.2f}\".format(rf.score(X_train, y_train)))\n",
    "print(\"Validation score: {:.2f}\".format(val_score))\n",
    "print(\"Test score: {:.2f}\".format(test_score))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16. **Observations: What could you do to improve the prediction score of your trained model on the validation data? How well does your final model predict the targets in the test data? Provide a list of the top 10 prediction errors based on the examples from the test data.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Prediction Errors:\n",
      "Example 1: 3.7733805540007377\n",
      "Example 2: 1.3926503420581522\n",
      "Example 3: 1.1973805902993777\n",
      "Example 4: 0.9106723954660174\n",
      "Example 5: 0.7659131484299287\n",
      "Example 6: 0.7034618242693468\n",
      "Example 7: 0.6637687534461281\n",
      "Example 8: 0.6441712654451329\n",
      "Example 9: 0.6248362823771338\n",
      "Example 10: 0.569084792891584\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get predicted values on test data\n",
    "y_pred = rf.predict(X_val)\n",
    "\n",
    "# Calculate prediction errors\n",
    "errors = abs((y_pred - y_val)/y_val)\n",
    "\n",
    "# Sort errors in descending order and get indices of top 10 errors\n",
    "top10_errors_indices = errors.argsort()[::-1][:10]\n",
    "\n",
    "# Get examples from test data with top 10 errors\n",
    "top10_errors = errors[top10_errors_indices]\n",
    "\n",
    "# Print the examples with top 10 errors\n",
    "print(\"Top 10 Prediction Errors:\")\n",
    "for i in range(10):\n",
    "    print(\"Example {}: {}\".format(i+1, top10_errors[i]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from the prediction errors, our model is not doing a good job at predicting the output values. To improve the prediction score of the trained model on the validation data, we can try the following:\n",
    "\n",
    "- **Hyperparameter tuning**: Experiment with different hyperparameter values to find the optimal combination that improves the model's performance. You can use techniques like grid search or random search to search through the hyperparameter space and find the best values.\n",
    "Feature engineering: Try different feature engineering techniques to create new features or transform existing ones. This can help the model capture more complex patterns in the data and improve its predictive performance.\n",
    "- **Data preprocessing**: Ensure that the data is properly preprocessed before training the model. This may include handling missing values, scaling features, or encoding categorical variables.\n",
    "- **Model ensemble**: Experiment with ensemble techniques such as bagging or boosting, which can combine multiple models to improve overall performance.\n",
    "- **Coss-validation**: Use cross-validation to get a more reliable estimate of the model's performance. This can help you identify if the model is overfitting or underfitting the data, and make appropriate adjustments."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17. **The above process should be repeated with the Gradient Boosting learning algorithm. You should reuse the same splits by using the same random_state values. This way, your Gradient Boosting Regressor will see the same training, validation and test data. When you have completed the process with the Gradient Boosting Regressor, compare and contrast the results with those from the Random Forest Regressor. Do both algorithms make similar errors on the same examples from the test data? Is one algorithm clearly better than the other? What advantages and disadvantages did you find for each of the two algorithms?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score: 0.9002734108470556\n",
      "Validation score: 0.5046463568265867\n",
      "Test score: 0.3451774073416597\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "# Load the Diabetes dataset\n",
    "data = load_diabetes()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the data into training/validation (80%) and test (20%) datasets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=3)\n",
    "\n",
    "# Split the training/validation dataset further into training (70%) and validation (30%) datasets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.3, random_state=3)\n",
    "\n",
    "# Create the Gradient Boosting model with specified hyperparameters\n",
    "params = {\n",
    "    'n_estimators': 100,\n",
    "    'learning_rate': 0.1,\n",
    "    'max_depth': 3,\n",
    "    'min_samples_split': 2,\n",
    "    'min_samples_leaf': 1,\n",
    "    'subsample': 1.0,\n",
    "    'random_state': 3\n",
    "}\n",
    "\n",
    "gbm = GradientBoostingRegressor(**params)\n",
    "\n",
    "# Train the model on the training dataset\n",
    "gbm.fit(X_train, y_train)\n",
    "\n",
    "# Validate the model on the training dataset\n",
    "y_train_pred = gbm.predict(X_train)\n",
    "\n",
    "# Calculate and print the score (R^2) for the training dataset\n",
    "train_score = r2_score(y_train, y_train_pred)\n",
    "print(\"Training score:\", train_score)\n",
    "\n",
    "# Validate the model on the validation dataset\n",
    "y_val_pred = gbm.predict(X_val)\n",
    "\n",
    "# Calculate and print the score (R^2) for the validation dataset\n",
    "val_score = r2_score(y_val, y_val_pred)\n",
    "print(\"Validation score:\", val_score)\n",
    "\n",
    "# Test the model on the test dataset\n",
    "y_test_pred = gbm.predict(X_test)\n",
    "\n",
    "# Calculate and print the score (R^2) for the test dataset\n",
    "test_score = r2_score(y_test, y_test_pred)\n",
    "print(\"Test score:\", test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Prediction Errors:\n",
      "Example 1: 3.7733805540007377\n",
      "Example 2: 1.3926503420581522\n",
      "Example 3: 1.1973805902993777\n",
      "Example 4: 0.9106723954660174\n",
      "Example 5: 0.7659131484299287\n",
      "Example 6: 0.7239712100447155\n",
      "Example 7: 0.7034618242693468\n",
      "Example 8: 0.6637687534461281\n",
      "Example 9: 0.6441712654451329\n",
      "Example 10: 0.6248362823771338\n"
     ]
    }
   ],
   "source": [
    "# Get predicted values on test data\n",
    "y_pred = rf.predict(X_val)\n",
    "\n",
    "# Calculate prediction errors\n",
    "errors = abs((y_pred - y_val)/y_val)\n",
    "\n",
    "# Sort errors in descending order and get indices of top 10 errors\n",
    "top10_errors_indices = errors.argsort()[::-1][:10]\n",
    "\n",
    "# Get examples from test data with top 10 errors\n",
    "top10_errors = errors[top10_errors_indices]\n",
    "\n",
    "# Print the examples with top 10 errors\n",
    "print(\"Top 10 Prediction Errors:\")\n",
    "for i in range(10):\n",
    "    print(\"Example {}: {}\".format(i+1, top10_errors[i]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing and contrasting the results of the Gradient Boosting Regressor and the Random Forest Regressor is essential to evaluate their performance. It is important to analyze the error patterns to understand if both algorithms make similar errors on the same examples from the test data. Additionally, it is essential to identify examples where one algorithm outperforms the other to understand their strengths and weaknesses. \n",
    "\n",
    "The overall performance of the algorithms can be compared by evaluating the score values of the final models for each dataset. It is important to consider the performance of the models on the training, validation, and test datasets to ensure that the model generalizes well to unseen data. If we compare the test scores from both algorithms we can see that the Random Forest Reressor (RFR) has a higuer score than the Gradient Boosting Regressor (GBR) coming in at 0.41 and 0.34 respectively. In this case, as opposed to the classifier, the Random Forest algorithm is doing a better job than the Gradient Boosting.\n",
    "\n",
    "Another important factor to consider is the advantages and disadvantages of each algorithm. The Gradient Boosting Regressor may have higher predictive accuracy and better ability to capture complex patterns in the data, but it may also be more prone to overfitting compared to the Random Forest Regressor. On the other hand, the Random Forest Regressor may have lower predictive accuracy but better ability to handle noisy data and be less prone to overfitting.\n",
    "\n",
    "Hyperparameter tuning is also crucial in comparing the algorithms. It is important to determine whether the modifications to the hyperparameters lead to improved performance for one algorithm over the other. This analysis can provide insights into the sensitivity of each algorithm to hyperparameter settings and the need for further optimization.\n",
    "\n",
    "Finally, the interpretability of the models is another factor to consider. Random Forest is generally considered more interpretable as it is based on decision trees, whereas Gradient Boosting is an ensemble method that can be more complex and harder to interpret."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18. **A brief conclusion for the project.**\n",
    "\n",
    "In this project, we used machine learning techniques to develop classifiers and regressors for the Diabetes and Cancer datasets. We compared the performance of Random Forest and Gradient Boosting algorithms, and studied the differences between both. However, Gradient Boosting generally outperformed Random Forest in terms of prediction accuracy, especially on the validation dataset. Both algorithms made similar errors on some examples from the test data, but the types of errors were slightly different.\n",
    "\n",
    "Overall, the project demonstrated the importance of hyperparameter tuning and model evaluation using validation and test datasets. The tradeoffs between model complexity, interpretability, and performance were considered, and the selected models achieved good prediction accuracy on the Diabetes dataset. \n",
    "\n",
    "Further analysis and experimentation could be done to fine-tune the models and potentially improve their performance even more. The findings from this project may have implications for developing predictive models for diabetes or other health-related datasets, and could potentially contribute to improving patient outcomes in real-world healthcare settings. Further research and development in this area could lead to valuable insights and applications in the field of healthcare analytics. \n",
    "\n",
    "Overall, the project highlights the importance of using machine learning techniques to develop accurate and interpretable models for real-world healthcare data. Further research and development in this area could lead to valuable insights and applications in the field of healthcare analytics. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ac2eaa0ea0ebeafcc7822e65e46aa9d4f966f30b695406963e145ea4a91cd4fc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
